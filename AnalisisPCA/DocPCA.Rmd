---
output:
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Declaración del problema

El objetivo del presente análisis es resumir y visualizar la información útil de todo el conjunto de datos con una pérdida mínima de la información, ya que hacer un análisis de 112 variables sería muy complicado. Para ello primero se observará la correlación de los datos, pues **si los datos están altamente correlacionados, significa que hay redundancia y que pueden ser reducidos a un conjunto más pequeño de datos (dimensiones)**. Después se construirán dichas dimensiones para que puedan ser usadas en un posterior análisis.

Después, con un conjunto de datos más sencillo, se separará dicho conjunto en 2; unos para construir un modelo de predicción para la variable *K*, y otro para validar dicho modelo. Con esto, se espera predecir la criticidad del reactor (*K*) con base a los isótopos utilizados.

## Descripción del conjunto de datos

El conjunto de datos sobre isótopos de carbono es un conjunto de datos multivariados de valor real que describe la criticidad (*K*) de un reactor y la potencia de este (*LP*).

Hay ciento doce columnas. Las primeras 110 se refieren a isótopos de carbono, donde *C* hace referencia al Carbono, el número después de la *C* a la cantidad de protones y neutrones en su núcleo, *U* a que es *inestable*, y *G* a que es emisor de *radiación gamma*. Por último, la columna *K* se refiere a la criticidad del reactor, y la columna *LP* a la potencia de este.

# Análisis

## Importación del conjunto de datos

```{r}
library("readxl")
library('corrr')
library("ggcorrplot")
library("FactoMineR")
library("factoextra")
```

Vista previa del conjunto de datos.

```{r}
datos <- read_excel("dataframe_inicial.xlsx")
# str(datos)
# Primeras 10 columnas
head(capture.output(str(datos)), n = 10)
```

...

```{r}
# Últimas 10 columnas
tail(capture.output(str(datos)), n = 10)
```


Nota: no se muestra la estructura de todas las columnas ya que son demasiadas.

## Limpieza de los datos

En esta etapa, debe asegurarse de deshacerse de todas las inconsistencias, como valores faltantes y variables redundantes.

Verificamos si hay valores nulos.

```{r}
colSums(is.na(datos))
```

No hay datos nulos.

Por otra parte, previamente se identificaron columnas con valores 0, por lo tanto, se decide no tomarlas en cuenta para el estudio ya que no aportan nada a este.

```{r}
datos = datos[, which(colSums(datos) != 0)]
# str(datos)
# Primeras 10 columnas
head(capture.output(str(datos)), n = 10)
```
...

```{r}
# Últimas 10 columnas
tail(capture.output(str(datos)), n = 10)
```


Con esto, nos quedan 57 columnas.

## Normalización de los datos

Con el fin de que las variables estén dentro del mismo rango (escala) y contribuyan igualmente al análisis, se someten a un proceso de estandarización.

Primero, se descartan las columnas *K* y *LP* por no ser relevantes en este análisis.

```{r}
numerical_data <- datos[,0:55]
# str(numerical_data)
# Primeras 10 columnas
head(capture.output(str(numerical_data)), n = 10)
```

...

```{r}
# Últimas 10 columnas
tail(capture.output(str(numerical_data)), n = 10)
```


Se estandarizan los datos:

```{r}
data_normalized <- scale(numerical_data)
head(data_normalized, n = 3)
```

Debido a que ciertas columnas contenían el mismo valor en todos sus registros, el resultado de la estandarización es nulo.
A continuación, se identificaron datos nulos en las columnas *C11U*, *C32G*, *C82G*, *C93G* y *C98G*, por lo que no se tomarán en cuenta.

```{r}
colSums(is.na(data_normalized))
```

Una vez identificadas dichas columnas, se eliminan del conjunto de datos.

```{r}
data_normalized = data_normalized[, colSums(is.na(data_normalized)) == 0]

head(data_normalized, n = 3)
```

## Computar la matriz de correlación

Ya con los datos limpios y estandarizados, se muestra la matriz de correlación de la *Figura 1*.

```{r echo = FALSE}
corr_matrix <- cor(data_normalized)
ggcorrplot(corr_matrix) +
  theme(axis.text.x = element_text(size = 6, angle = 90),
        axis.text.y = element_text(size = 6)) +
  ggtitle("Figura 1. Matriz de correlación") +
  theme(plot.title = element_text(hjust = 0.5))
```

Correlación positiva o negativa.

## PCA

Se "construyen" nuevas variables a partir de combinaciones lineales o mezclas de las variables iniciales.
A continuación se presenta un resúmen de las componentes genreadas.

```{r}
datos.pca <- princomp(corr_matrix)
summary(datos.pca)
```

Se presentan, en detalle, las primeras 2 componentes generadas. Estas representan un mayor porcentaje de los datos originales, como se verá a continuación.

```{r}
datos.pca$loadings[, 1:2]
```

Como se muestra en la gráfica de la *Figura 2*, el primer componente contiene la máxima información posible de las variables, representándolas en hasta un 62.6%

```{r echo = FALSE}
fviz_eig(datos.pca, addlabels = TRUE) +
  ggtitle("Figura 2. Porcentaje de representación de los componentes principales") +
  theme(plot.title = element_text(hjust = 0.5))
```

De forma geométrica se muestra en la *Figura 3* que los componentes principales representan las direcciones de los datos que explican la máxima cantidad de varianza. En el caso del primer componente se aprecia que, en la siguiente gráfica:

- las *líneas* que representan a las variables originales tienden a agruparse en el eje *x*, correspondiente a la *dimensión 1*.

- a mayor distancia entre las variables y el origen, mejor es su representación.

```{r echo = FALSE}
fviz_pca_var(datos.pca, col.var = "blacK") +
  ggtitle("Figura 3. Variables - PCA") +
  theme(plot.title = element_text(hjust = 0.5))
```

Se observa que en el eje horizontal *Dim1* muchas variables están correlacionadas, tales como *C98U*, *C108U*, *C72U*...

### Calidad de la representación

Así mismo podemos ver la calidad de la representación de las variables originales en los componentes principales obtenidos, como se muestra en la *Figura 4*.

```{r echo = FALSE}
fviz_cos2(datos.pca, choice = "var", axes = 1:2) +
  theme(axis.text.x = element_text(size = 8)) +
  ggtitle("Figura 4. Calidad de la representación") +
  theme(plot.title = element_text(hjust = 0.5))
```

Por ejemplo, el isótopo *C98U* es bien representado en la componente principal, seguido de los isótopos *C101U* y *C92*.

Finalmente podemos ver la calidad de representación de cada variable en el siguiente gráfico (*Figura 5*) a través de un esquema de colores.

- Entre más verde sea el color de la variable, mejor está representada en los componentes obtenidos.

- Entre más alejada esté una variable del origen, mejor será su representación.

- Las variables cerca del origen son menos importantes para los primeros componentes obtenidos.

```{r echo = FALSE}
fviz_pca_var(datos.pca, col.var = "cos2",
            gradient.cols = c("black", "orange", "green"),
            repel = TRUE) +
  ggtitle("Figura 5. Variables - PCA") +
  theme(plot.title = element_text(hjust = 0.5))
```

Con lo que podemos afirmar que la mayoría de las variables son bien representadas con los componentes *Dim1* y *Dim2*.

### Conclusión PCA

Gracias al análisis de componentes principales aplicado se consiguió reducir la complejidad del conjunto de datos inicial y se comprobó que esta reducción representa a la mayoría de las variables iniciales, con *Dim1* representando en hasta un 62.6% las variables originales y que la variable *C98U* es la mejor representada por *Dim1*.
Obteniendo que las nuevas variables *Dim1* y *Dim2* son aptas para un posterior estudio.


## OverSampling

Una vez reducida la complejidad del conjunto de datos al realizar el análisis PCA, se puede usar la primera componente principal obtenida (*que representa un 62.6% de las variables originales*).

```{r}
library(ROSE)
```

A continuación se muestran los registros de dicha componente.

```{r}
datos.pca$loadings[, 1]
```

```{r}
# prop.table(table(datos.pca$loadings[, 1]))
table(datos.pca$loadings[, 1])
```



### Grafica de distribución de clases

Para apreciar de forma gráfica la distribución de clases se construye la gráfica de la *Figura 6*.

```{r}
barplot(prop.table(table(datos.pca$loadings[,1])),
        col = rainbow(2),
        ylim = c(0, 1),
        main = "Figura 6. Distribución de clases")
```

Como se muestra en la *Figura 6*, los datos están balanceados, ya que cada registro es único y hace referencia a las columnas iniciales válidas (las resultantes después de la limpieza del conjunto) del conjunto de datos.

## Modelo de predicción

A continuación se preparan los datos que serán usados como datos de entrenamiento y de prueba, para ello se determina que el 80% de los datos se usarán para entrenar el modelo; el resto será usado para validarlo.

```{r}
library(caret)
split <- 0.8  # porcentaje de datos al conjunto de entrenamiento
set.seed(123)

datos_modelo <- datos.pca$loadings

trainIndex <- createDataPartition(datos_modelo[, 1], p = split, list = FALSE)
```

Se generan los conjuntos de entrenamiento y testeo del modelo.

```{r}
datos_train <- datos_modelo
datos_test <- datos_modelo[-trainIndex, ]
```

### Entrenamiento

Se estandarizan los datos antes de usarlos para predecir.

```{r}
k_values <- (datos[, ncol(datos) - 1] - min(datos[, ncol(datos) - 1])) / (max(datos[, ncol(datos) - 1]) - min(datos[, ncol(datos) - 1]))
k_values <- k_values[1:50,]

Comp1 <- datos_train[,1]

train <- data.frame(Comp1, k_values)[1:50,]

datos_ajuste <- glm(k_values ~ ., data = train, family = binomial(link = "logit"))

summary(datos_ajuste)
```

Se realiza la predicción con el modelo.

```{r}
datos_predecidos <- predict(datos_ajuste, newdata = data.frame(datos_test), type = "response")
head(datos_predecidos)
```

Para validar el modelo se genera la matriz de correlación de la *Figura 7* que compara los valores reales y los predecidos por el modelo. Podemos observar que en la mayoría de los casos hay una ffuerte correlacion, por lo que podemos asegurar que el modelo está bien entrenado.

```{r echo=FALSE}
#Testing model
#table(k_values, datos_predecidos)
actuals_preds <- data.frame(cbind(actuals=k_values, predicted=datos_predecidos))

ggcorrplot(actuals_preds) +
  theme(axis.text.x = element_text(size = 10, angle = 0),
        axis.text.y = element_text(size = 10)) +
  ggtitle("Figura 7. Matriz de correlación valores reales y predecidos") +
  theme(plot.title = element_text(hjust = 0.8, vjust = 20))

```


# Conclusión

En el presente análisis se ha disminuido la complejidad del conjunto de datos, lo que ha facilitado la creación de un modelo de predicción de la criticidad de un reactor (*K*) bajo ciertas condiciones. Así, se consiguió trabajar con una sola variable (la componente principal número 1), en vez de las 110 variables iniciales.
Finalmente, podemos asegurar que el modelo propuesto es válido para su uso ya que los valores predecidos y los realles son muy cercanos (Véase la *Figura 7*).

